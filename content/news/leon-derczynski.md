---
title: "Keynote Leon Derczynski"
date: 2021-02-19T16:05:25+01:00
---

<style>
    img.first_image {
        max-width: 100%; 
        max-height: 400px;
        display: block;
        margin-left: auto;
        margin-right: auto;
    }
    
    .abstract {
        text-align: justify;
        font-size:0.8rem;
        padding: 1rem;
        background-color: rgba(96,24,67,0.03);
        border-left: 4px solid #2c5282;
        border-radius: 0 6px 6px 0;
    }
</style>

<img class="first_image" src="/images/news/leon-derczynski.jpg" alt = "Portrait of Leon Derczynski">

## About Leon
We are happy to announce that Professor Leon Derczynski, principal research scientist in LLM Security at [NVIDIA](https://www.nvidia.com/en-us/)
and prof in Natural Language Processing (NLP)
at [ITU Copenhagen](https://en.itu.dk/) (IT University of Copenhagen) will give a keynote speech at the Computational Humanities Research 2024 Conference in Aarhus, Denmark.

Professor Derczynski contributes to leading bodies on LLM securing, being on the OWASP LLM Top 10 core team, working on ML Commons, and founder 
of the ACL SIG on NLP Security.

He has published over a hundred NLP papers and his recent talks at DEF CON, demonstrating LLM security technology developed with NVIDIA, 
were standing room only. Professor Derczynski heads up garak, LLM vulnerability scanner, in the NeMo Guardrails team at NVIDIA.

## The Keynote
Title: *"What Computer Science Can’t Fix About LLM Security"*

<section>
<h3>Abstract</h3>
<div class="abstract">
<p>In the words of Gould et. al: “Interactive large language models (LLMs) are so hot right now, and are probably going to be hot for a while”. The use of text generation systems in contexts far beyond the task they are trained for - modelling language - leads directly to novel and unbounded risks in safety and security. While some of these risks can be mitigated some of the time, the sheer size of this wildly creative frontier of failures presents an interesting challenge, even in terms of rough enumeration. This talk takes a qualitative approach to LLM security in the current generation of models, showing the crucial artisanal role of the human (or even the computer scientist) in this traditionally highly computational area. From this, we’ll attempt work out what security and safety issues we have some hope of reasonably addressing automatically - and what we might best continue to leave to the humans.</p>
</div>